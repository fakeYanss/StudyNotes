### 激活函数

* sigmoid（0.1）

  神经网络中常用的的激活函数，当梯度太大导致激活函数弥散，即梯度小时，叫做神经元饱和

  sigmod函数，是逻辑斯蒂回归的压缩函数，它的性质是可以把分隔平面压缩到[0,1]区间一个数（向量），在线性分割平面值为0时候正好对应sigmod值为0.5，大于0对应sigmod值大于0.5、小于0对应sigmod值小于0.5；0.5可以作为分类的阀值；exp的形式最值求解时候比较方便，用相乘形式作为logistic损失函数，使得损失函数是凸函数；不足之处是sigmod函数在y趋于0或1时候有死区，控制不好在bp形式传递loss时候容易造成梯度弥撒。 

* tahn（-1，1）

* relu（0，1）

  能够保证梯度在正向时输出值与原始值一样。

  但是当输出为负，也会神经元饱和

  

  **前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。 **