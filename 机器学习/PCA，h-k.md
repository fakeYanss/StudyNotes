### PCA

pca提取的是数据分布方差比较大的方向

隐藏层可以提取有预测能力的特征



**降维的作用**

①数据在低维下更容易处理、更容易使用；
②相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示；
③去除数据噪声
④降低算法开销

**降维通俗点的解释**

一些高维度的数据，比如淘宝交易数据，为便于解释降维作用，我们在这假设有下单数，付款数，商品类别，售价四个维度，数据量上百万条，对于下单数和付款数，我们可以认为两者是线性相关的，即知道下单数，我们可以得到付款数，这里很明显这两个属性维度有冗余，去掉下单数，保留付款数，明显能再保证原有数据分布和信息的情况下有效简化数据，对于后面的模型学习会缩短不少时间和空间开销。这就是降维，当然并不是所有数据中都会有过于明显线性相关的属性维度，我们降维后最终的目标是各个属性维度之间线性无关。

**PCA降维步骤原理**

首先既然要度量那些是否存在相关的属性，我们就要用到协方差，在博客相关分析中有介绍，这里不再赘述，协方差衡量的是2维属性间的相关性，对于n个维度的属性，就需要协方差矩阵，其对角线为各维度的方差。

步骤：

​                       **设有m条n维数据。**

​                      **1）将原始数据按列组成n行m列矩阵X**

​                      **2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值**

​                      **3）求出协方差矩阵**

​                      **4）求出协方差矩阵的特征值及对应的特征向量r**

​                      **5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P**

​                      **6）** **即为降维到k维后的数据**



**关于维数k的选择**

使用一个公式error=![img](http://img.my.csdn.net/uploads/201209/28/1348845556_8584.png)，表示压缩后的误差，m所有特征的个数，然后确定一个阈值x，比如0.01，选取一个K，使得error < x则我们认为这个m可以接受，否则尝试其他.



### H-K算法

基于二次准则函数的H-K算法较之于感知器算法的优点是：可以判别问题是否线性可分

* 适用于线性可分与非线性可分的情况
* 对于线性可分的情况，给出最优权矢量
* 对于非线性可分得情况，能够判别出来，退出迭代过程



算法思想：**就是在最小均方误差准则下求得权矢量。**



