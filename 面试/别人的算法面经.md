##### 1.gbdt,xgboost,lgbm的区别(阿里，头条)

* gbdt。一阶泰勒展式，通过计算梯度来定位模型不足。
* xgboost。二阶泰勒展式，目标函数多了正则项，防止过拟合和处理缺省值。
* lbgm。lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。 

##### 2.梯度下降法，牛顿法，拟牛顿法区别(阿里)

* 牛顿法，二阶收敛，二阶曲面拟合，收敛快，hessain矩阵的逆矩阵计算复杂
* 拟牛顿法，hessian矩阵的近似

##### 3.SGD,ADAM区别(百度)

* SGD,一阶收敛，平面拟合，收敛慢
* ADAM,利用梯度一阶矩阵和二阶矩阵估计动态调整，经过偏置校正，每次迭代学习率有个确定范围。

##### 4.什么是梯度消失，饱和，如何改善(阿里) 

* 深度神经网络中梯度不稳定，前面层中的梯度=后面层上梯度的乘积，会消失（梯度在0到1）或爆炸（梯度大于1）
* 梯度爆炸，初始权值过大，前面层比后面层变化的更快，导致权值越来越大
* 梯度消失，前面层比后面层梯度变化更小，故变化慢，引起梯度消失。sigmoid

##### 5.lr的推导(腾讯)

##### 6.SVM目标函数，为什么转为对偶(腾讯，百度)

* 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
* 方便核函数的引入
* 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。

##### 7.定义class mlp(头条)

##### 8.kd tree(腾讯)

#####  

##### 9.FFM的优化(百度)  

##### 10.解释RESNET(百度，阿里)

##### 11.mapreduce思想(腾讯)

##### 12.解释BN(头条，百度) 

##### 13.非结构化文本处理方法(阿里) 

##### 14.bagging.boosting.stacking区别(阿里) 

集成学习

- 用于减少方差的bagging
- 用于减少偏差的boosting
- 用于提升预测结果的stacking

##### 15.CNN与RNN的区别(阿里)  

* DNN，又叫多层感知机，可以理解为有很多隐藏层的神经网络。有以下局限
  * 参数数量膨胀。DNN采用全连接。结构中连接带来数量级全职参数，易过拟合，陷入局部最优。
  * 局部最优。网络加深，优化函数更易陷入局部最优。
  * 梯度消失。使用sigmoid激活函数（传递参数），在BP反向传播梯度式，梯度会衰减，随着网络加深，衰减累计，到底层梯度基本为0.
  * 无法对时间序列上的变化进行建模

* cnn：局部连接，权值共享，池化操作，多层操作。

  * 针对DNN存在的参数数量膨胀问题，并非所有cnn的上下神经元都直接相连，通过卷积核作为中介。
  * 同一卷积核共享，图像通过卷积操作仍能保留原先的位置关系。
  * cnn之所以适合图像识别，也是因为cnn限制参数个数并挖掘局部结构这个特点。

* RNN针对cnn中无法对时间序列变化进行建模的局限，处理时序数据。

  * cnn中每层神经元只向上一层传播，样本处理在各个时刻独立，及前馈神经网络。
  * RNN中，神经元输出可以在下一个时间戳直接作用到自身。比如（T+1）时刻的输出就是该时刻的输入和所有历史共同作用的结果，即对时间序列建模
  * RNN可被看作是在一个时间上传递的神经网络，他的深度是时间的长度，梯度小时就出现在时间轴上。

* LSTM：为解决RNN中时间上的梯度消失，通过门的开关实现时间上记忆的功能，房子梯度消失。

* one to one：表示的是CNN网络的场景，从固定的输入到固定的输出

  one to many：RNN的场景，序列输出，有点像看图说话，例如固定了输入的图片，然后输出一段序列描述这个图的意义

  many to one：RNN的场景，序列输入，比如我们做语义情感分析，输入一串不定长度的话，返回情绪

  many to many：RNN的场景，常见的sequence to sequence，比如之前的一个文章到的，通过周杰伦的歌词数据，模仿写出一首周杰伦风格的歌词，这种场景的输入和输出的长度都是不定的。

##### 16.如何防止过拟合(头条) 

* dropout。 

  Dropout方法是通过修改ANN中隐藏层的神经元个数来防止ANN的过拟合。 

  神经网络中通过修改网络自身结构来实现的一种方法。对网络进行训练时随机删除一些隐藏神经元，同时保证输入层与输出层神经元个数不变。

* 正则化。通过在代价函数后面加正则项来防止模型过拟合。

  正则项是为了降低模型的复杂度，从而避免模型区过分拟合训练数据，包括噪声与异常点（outliers）。从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。

  *  L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。
  * ​    它们都是服从均值为0，协方差为1|λ。当λ=0λ时，即没有先验）没有正则项，则相当于先验分布具有无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练集数据， 参数ww可以变得任意大从而使得模型不稳定，即方差大而偏差小。λλ越大，标明先验分布协方差越小，偏差越大，模型越稳定。即，加入正则项是在偏差bias与方差variance之间做平衡tradeoff（来自知乎）。 
  * L1正则项是为了使得那些原先处于零（即|w|≈0）附近的参数ww往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。  

* early  stopping。

  * Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。 
  * Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。 

* 数据集扩增

  * 从数据源头采集更多数据

  * 复制原有数据并加上随机噪声
  * 重采样
  * 根据当前数据集估计数据分布参数，使用该分布产生更多数据等