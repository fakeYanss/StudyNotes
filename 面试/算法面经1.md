##### 1.gbdt,xgboost,lgbm的区别(阿里，头条)

* gbdt，一阶泰勒展式，通过计算梯度来定位模型不足
* xgboost，二阶泰勒展式，目标函数多了正则项，防止过拟合，处理缺省值。
* lgbm，lightgbm支持直接输入categorical 的feature  在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。  

##### 2.梯度下降法，牛顿法，拟牛顿法区别(阿里)

* SGD,一阶收敛，平面拟合，收敛慢
* 牛顿法，二阶收敛，二阶曲面拟合，收敛快，但是hessian矩阵求逆计算复杂
* 拟牛顿法，hessian矩阵的近似
* Adam,利用梯度的一阶矩阵和二阶矩阵估计动态调整
  * 优点，经过偏置矫正后，每次迭代学习率都有个确定范围，使参数平稳
  * 对内存需求小，也适用大多非凸优化，大数据及和高维空间

##### 3.SGD,ADAM区别(百度)

##### 4.什么是梯度消失，饱和，如何改善(阿里) 

其中，梯度消失爆炸的解决方案主要包括以下几个部分。

1. - **预训练加微调**
2. **- 梯度剪切、权重正则（针对梯度爆炸）**
3. **- 使用不同的激活函数**
4. **- 使用batchnorm**
5. **- 使用残差结构**
6. **- 使用LSTM网络**

梯度消失与梯度爆炸其实是一种情况两种情况下梯度消失经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。梯度爆炸一般出现在深层网络和**权值初始化值太大**的情况下

因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，本质在于方法问题，另外多说一句，对于人来说，在大脑的思考机制里是没有反向传播的， 

##### 5.lr的推导(腾讯)

极大似然估计-log损失函数-参数更新-softmax损失函数

##### 6.SVM目标函数，为什么转为对偶(腾讯，百度)

* 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
* 方便核函数的引入
* 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。

##### 7.定义class mlp(头条)

##### 8.kd tree(腾讯)

##### 

##### 9.FFM的优化(百度)  

##### 10.解释RESNET(百度，阿里)

##### 11.mapreduce思想(腾讯)

##### 12.解释BN(头条，百度) 

##### 13.非结构化文本处理方法(阿里) 

##### 14.bagging.boosting.stacking区别(阿里)  

* https://blog.csdn.net/data_scientist/article/details/79022025

##### 15.CNN与RNN的区别(阿里)  

* cnn：局部连接，权值共享，池化操作，

* one to one：表示的是CNN网络的场景，从固定的输入到固定的输出

  one to many：RNN的场景，序列输出，有点像看图说话，例如固定了输入的图片，然后输出一段序列描述这个图的意义

  many to one：RNN的场景，序列输入，比如我们做语义情感分析，输入一串不定长度的话，返回情绪

  many to many：RNN的场景，常见的sequence to sequence，比如之前的一个文章到的，通过周杰伦的歌词数据，模仿写出一首周杰伦风格的歌词，这种场景的输入和输出的长度都是不定的。



##### 16.如何防止过拟合(头条) 

* dropout。 

  Dropout方法是通过修改ANN中隐藏层的神经元个数来防止ANN的过拟合。 

  神经网络中通过修改网络自身结构来实现的一种方法。对网络进行训练时随机删除一些隐藏神经元，同时保证输入层与输出层神经元个数不变。

* 正则化。通过在代价函数后面加正则项来防止模型过拟合。

  正则项是为了降低模型的复杂度，从而避免模型区过分拟合训练数据，包括噪声与异常点（outliers）。从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。

  *  L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。
  * ​    它们都是服从均值为0，协方差为1|λ。当λ=0λ时，即没有先验）没有正则项，则相当于先验分布具有无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练集数据， 参数ww可以变得任意大从而使得模型不稳定，即方差大而偏差小。λλ越大，标明先验分布协方差越小，偏差越大，模型越稳定。即，加入正则项是在偏差bias与方差variance之间做平衡tradeoff（来自知乎）。 
  * L1正则项是为了使得那些原先处于零（即|w|≈0）附近的参数ww往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定），从而防止过拟合，提高模型的泛化能力。  

* early  stopping。

  * Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。 
  * Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。 

* 数据集扩增

  * 从数据源头采集更多数据

  * 复制原有数据并加上随机噪声
  * 重采样
  * 根据当前数据集估计数据分布参数，使用该分布产生更多数据等